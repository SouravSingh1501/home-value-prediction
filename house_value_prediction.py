# -*- coding: utf-8 -*-
"""House Value Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DH7HqiKT-TAOx2aIXSDxfWVPnSgdGnsG
"""

# Importing Required Libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

data=pd.read_csv('/PROECT/Metropolitan.csv')

data.head()

data.describe()

# There is a house with 9 bedrooms!, would be an interesting find
# The area ranges from 200 sqft to 16,000 sqft. (The data has good distribution)

data['No. of Bedrooms'].value_counts().plot(kind='bar')
plt.title('Number of bedrooms')
plt.xlabel('Bedrooms')
plt.ylabel('Count')

# 2 and 3 bedroom homes (2BHK and 3BHK) are the most popular

data.columns

plt.scatter(data.Price, data.Area)
plt.title('Price vs Square Footage')

plt.scatter(data['Price'], data['City'])
plt.title('Price in Cities')

data.nunique()

data.drop(columns=['Resale',
'MaintenanceStaff',
'Gymnasium',
'SwimmingPool',
'LandscapedGardens',
'JoggingTrack',
'IndoorGames',
'Intercom',
'SportsFacility',
'ClubHouse',
'StaffQuarter',
'Cafeteria',
'MultipurposeRoom',
'WashingMachine','Gasconnection','AC','Wifi',
"Children'splayarea",
'LiftAvailable',
'BED','VaastuCompliant',
'Microwave', 'GolfCourse','TV',
'DiningTable','Sofa','Wardrobe','Refrigerator' ], inplace=True)

data.columns

data.drop(columns=['Location'], inplace=True)

data.head()

data.groupby('City')['City'].agg('count')

data.isnull().sum()

# No null values present

data=data[data['No. of Bedrooms'] < 5]

data['No. of Bedrooms'].unique()

# Removed homes that have bedrooms more than 4
# As they are insignificant in the dataset

data=data[data['Area'] <= 9000]

data=data[data['Price'] < 40000000]

data['Area'].min()

data['Area'].max()

data['Price'].min()

data['Price'].max()

plt.scatter(data.Price, data.Area)
plt.title('Price vs Square Footage')

sns.barplot(data=data, x='24X7Security', y='Price')

data.nunique()

data.drop_duplicates(keep='first', inplace=True, ignore_index=True)

data = data[data['ShoppingMall'] < 9]

sns.barplot(data=data, x='24X7Security', y='Price')

plt.scatter(data.Price, data.Area)
plt.title('Price vs Square Footage')

"""#### We had to cap the area distribution again to 5000 after 9000 because the individual rows with 9 as a value in a binary column were removed. Make it sound formally
#### It shows in the scatter plot wherein the max value on y-axis(namely square footage) is 5000
"""

data=data[data['Area'] <= 5000]

data.nunique()

data.describe()

data.head(100)

data.shape

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

enc = LabelEncoder()
data['EncCity'] = enc.fit_transform(data['City'])

data.corr()

plt.figure(figsize=(10,6))
sns.heatmap(data.corr(),annot=True)

sns.scatterplot(x=data['Price'], y=data['Area'])
plt.xlabel('Price')
plt.ylabel('Area')
plt.title('India Metropolitan Areas: Price vs Area');

from scipy.stats import pearsonr
pearsonr(data['Price'], data['Area'])

target = "Price"
feature = ["Area"]
X_train = data[feature]
y_train = data[target]

y_mean = y_train.mean()
y_pred_baseline = [y_mean] * len(y_train)


len(y_pred_baseline) == len(y_train)

from sklearn.metrics import mean_absolute_error
mae_baseline = mean_absolute_error(y_train, y_pred_baseline)

print("Mean apt price:", round(y_mean, 2))
print("Baseline MAE:", round(mae_baseline, 2))



y_mean - mae_baseline

""" ### The MAE error is Rs 9330108.25 whilst the Baseline MAE is Rs.5048512.09. This means that by following this Baseline model, we would be off by about Rs.4281596.15587042"""

import plotly.express as px
fig=px.bar(data_frame=data, x='No. of Bedrooms', y='Price', color='City',barmode='relative')
fig.update_layout(title=dict(text='Average house price variation across Cities', xanchor='center', yanchor='top', x=0.5))

fig=px.scatter(data_frame=data, x='Area', y='Price', color='City', hover_name='City')
fig.update_layout(title=dict(text='House price Vs. Area across Cities', xanchor='center', yanchor='top', x=0.5))
fig.show()

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

X = data[['Area','No. of Bedrooms','RainWaterHarvesting','School','ATM','ShoppingMall','24X7Security','PowerBackup','CarParking','Hospital','EncCity']]
y = data[['Price']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.11, random_state=42)

gbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=8, random_state=30)

gbr_model.fit(X_train, y_train)

y_pred = gbr_model.predict(X_test)

from sklearn.metrics import mean_squared_error,r2_score

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print("R_squared score is: ",r2_score(y_pred, y_test))

# model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
# model.fit(X_train, y_train)

# y_pred = model.predict(X_test)

# r2 = model.score(X_test, y_test)
# print("Mean_absoloute_score is: ", mean_absolute_error(y_pred, y_test))
# print("R_squared score is: ",r2_score(y_pred, y_test))

# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error
# import numpy as np

# # Generate a large synthetic dataset
# np.random.seed(42)
# X_large = np.random.rand(10000, 1)
# y_large = 2 * X_large.squeeze() + 1 + 0.1 * np.random.randn(10000)

# # Split the data into training and testing sets
# X_train_large, X_test_large, y_train_large, y_test_large = train_test_split(X_large, y_large, test_size=0.2, random_state=42)

# # Use subsample for training
# gbr_subsample = GradientBoostingRegressor(subsample=0.6, n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42)
# gbr_subsample.fit(X_train_large, y_train_large)

# # Make predictions on the test set
# predictions_subsample = gbr_subsample.predict(X_test_large)

# # Evaluate the model with Mean Squared Error
# mse_subsample = mean_squared_error(y_test_large, predictions_subsample)
# print(f"Mean Squared Error with Subsample: {mse_subsample}")

import numpy as np

# Assume 'gbr_model' is your trained Gradient Boosting Regressor model

# Get input from the user
area = float(input("Enter the Area: "))
bedrooms = int(input("Enter the Number of Bedrooms: "))
rain_water_harvesting = int(input("Does it have Rain Water Harvesting? (1 for Yes, 0 for No): "))
school = int(input("Is it near a School? (1 for Yes, 0 for No): "))
atm = int(input("Is it near an ATM? (1 for Yes, 0 for No): "))
shopping_mall = int(input("Is it near a Shopping Mall? (1 for Yes, 0 for No): "))
security_24x7 = int(input("Does it have 24x7 Security? (1 for Yes, 0 for No): "))
power_backup = int(input("Does it have Power Backup? (1 for Yes, 0 for No): "))
car_parking = int(input("Does it have Car Parking? (1 for Yes, 0 for No): "))
hospital = int(input("Is it near a Hospital? (1 for Yes, 0 for No): "))
city = int(input("Enter the City (as an encoded number): "))  # Assuming you have an encoded city value

# Create a numpy array with the user input
user_input = np.array([[area, bedrooms, rain_water_harvesting, school, atm, shopping_mall, security_24x7, power_backup, car_parking, hospital, city]])

# Use the model to make a prediction
predicted_price = gbr_model.predict(user_input)

# Display the predicted price
print(f"The predicted price for the given input is: {predicted_price[0]}")